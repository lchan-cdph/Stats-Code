---
title: "Machine Learning Demo"
author: "Lawrence Chan, PhD"
date: "2023-01-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
summary(diabetes)
```

```{r}
par(mfrow=c(2,2))
hist(diabetes$Pregnancies)
hist(diabetes$Glucose)
hist(diabetes$BloodPressure)
hist(diabetes$SkinThickness)
hist(diabetes$Insulin)
hist(diabetes$BMI)
hist(diabetes$DiabetesPedigreeFunction)
hist(diabetes$Age)
hist(diabetes$Outcome)
```

```{r}
boxplot(diabetes$BloodPressure,
        ylab = "BloodPressure"
)

boxplot(diabetes$Glucose,
        ylab = "Glucose"
)

boxplot(diabetes$BMI,
        ylab = "BMI"
)
```

```{r}
#DataViz
install.packages("ggplot2")
library(ggplot2)
ggplot(diabetes, aes(x=Glucose, y=BMI))+geom_point()
ggplot(diabetes, aes(x=Glucose, y=BMI, col=Age))+geom_point()
```

```{r}
ggplot(diabetes, aes(x=Glucose, y=BMI))+geom_point(aes(col=Age)) + geom_smooth(color="red")
ggplot(diabetes, aes(x=Glucose, y=BloodPressure))+geom_point(aes(col=Age)) + geom_smooth(color="red")
```

```{r}
ggplot(diabetes, aes(Glucose, BloodPressure)) + geom_point(color="blue") + stat_summary(fun.y = "mean", geom = "line", linetype = "dashed")

ggplot(diabetes, aes(Glucose, BloodPressure)) + geom_point(color="blue") + geom_rug(show.legend = FALSE) + stat_summary(fun.y = "mean", geom = "line", linetype = "dashed")
```

```{r}
install.packages("dplyr")
library(dplyr)
diabetes %>%
dplyr::group_by(Outcome) %>% 
  ggplot(aes(x = Glucose, y = Outcome)) + geom_col(aes(fill = Outcome), color = NA) + labs(x="", y ="Outcome") + coord_polar()
```

```{r}
install.packages("GGally")
library(GGally)
ggpairs(diabetes)
```
```{r}
ggplot(diabetes, aes(BloodPressure, Glucose)) + geom_density_2d_filled(show.legend = FALSE) + coord_cartesian(expand = FALSE) + labs(x = "BloodPressure")
ggplot(diabetes, aes(Glucose, BloodPressure)) + geom_density_2d_filled(show.legend = FALSE) + coord_cartesian(expand = FALSE) + labs(x = "Glucose")
ggplot(diabetes, aes(Glucose, Outcome)) + geom_density_2d_filled(show.legend = FALSE) + coord_cartesian(expand = FALSE) + labs(x = "Glucose")
```

```{r}
t.test(Glucose ~ Outcome, diabetes)
```

```{r}
ggplot(diabetes, aes(x=as.factor(Glucose), y = BloodPressure)) + geom_boxplot() + theme(panel.border = element_rect(fill = "transparent", color = "#72efdd", size = 4))
```

```{r}
mylogit <- glm(Outcome ~ Glucose + BloodPressure + BMI + Age, data = diabetes, family = "binomial")
summary(mylogit)
confint.default(mylogit)
#bloodpressure is not as strong a predictor given the data, CI includes 0 
```

```{r}
# ensure the results are repeatable
set.seed(7)
# load the library
install.packages("mlbench")
library(mlbench)
install.packages("caret")
library(caret)
```

```{r}
# calculate correlation matrix
correlationMatrix <- cor(diabetes[,1:8])
# summarize the correlation matrix
print(correlationMatrix)
# find attributes that are highly corrected (ideally >0.75)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5)
# print indexes of highly correlated attributes
print(highlyCorrelated)
```

```{r}
# ensure results are repeatable
set.seed(7)
# load the library
library(mlbench)
library(caret)
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
model <- train(Outcome~., data=diabetes, method="glm", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(model, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)
```
```{r}
# ensure the results are repeatable
set.seed(7)
# load the library
library(mlbench)
library(caret)
install.packages("randomForest")
library(randomForest)
# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(diabetes[,1:8], diabetes[,9], sizes=c(1:8), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
```

```{r}
stats::kmeans(diabetes, centers = 3, nstart = 10)
```

```{r}
set.seed(12)
init <- sample(3, nrow(diabetes), replace = TRUE)
plot(diabetes, col = init)
```

```{r}
par(mfrow = c(1, 2))
plot(diabetes, col = init)
centres <- sapply(1:3, function(i) colMeans(diabetes[init == i, ], ))
centres <- t(centres)
points(centres[, 1], centres[, 2], pch = 19, col = 1:3)

tmp <- dist(rbind(centres, diabetes))
tmp <- as.matrix(tmp)[, 1:3]

ki <- apply(tmp, 1, which.min)
ki <- ki[-(1:3)]

plot(diabetes, col = ki)
points(centres[, 1], centres[, 2], pch = 19, col = 1:3)
```

```{r}
cl1 <- kmeans(diabetes, centers = 3, nstart = 10)
cl2 <- kmeans(diabetes, centers = 3, nstart = 10)
table(cl1$cluster, cl2$cluster)
```

```{r}
cl1 <- kmeans(diabetes, centers = 3, nstart = 1)
cl2 <- kmeans(diabetes, centers = 3, nstart = 1)
table(cl1$cluster, cl2$cluster)
```

```{r}
set.seed(42)
xr <- matrix(rnorm(prod(dim(diabetes))), ncol = ncol(diabetes))
cl1 <- kmeans(xr, centers = 3, nstart = 1)
cl2 <- kmeans(xr, centers = 3, nstart = 1)
table(cl1$cluster, cl2$cluster)
```

```{r}
diffres <- cl1$cluster != cl2$cluster
par(mfrow = c(1, 2))
plot(xr, col = cl1$cluster, pch = ifelse(diffres, 19, 1))
plot(xr, col = cl2$cluster, pch = ifelse(diffres, 19, 1))
```